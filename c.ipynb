{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sqlalchemy aiosqlite aiocache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from aiocache import Cache, cached\n",
    "from aiocache.serializers import JsonSerializer, PickleSerializer\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Result\n",
    "from sqlalchemy.exc import MultipleResultsFound, NoResultFound\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import Select\n",
    "from sqlalchemy.sql.elements import TextClause\n",
    "from sqlalchemy.sql.expression import FunctionElement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "class CacheConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache_type: str = \"redis\",\n",
    "        endpoint: str = \"127.0.0.1\",\n",
    "        port: int = 6379,\n",
    "        db: int = 0,\n",
    "        serializer: str = \"json\",\n",
    "        ttl: int = 3600,  # Default TTL: 1 hour\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        self.cache_type = cache_type\n",
    "        self.endpoint = endpoint\n",
    "        self.port = port\n",
    "        self.db = db\n",
    "        self.ttl = ttl\n",
    "        self.serializer = JsonSerializer() if serializer == \"json\" else PickleSerializer()\n",
    "        self.aiocache_kwargs = kwargs\n",
    "\n",
    "        if cache_type == \"redis\":\n",
    "            self.cache_class = Cache.REDIS\n",
    "        elif cache_type == \"memcached\":\n",
    "            self.cache_class = Cache.MEMCACHED\n",
    "        elif cache_type == \"memory\":\n",
    "            self.cache_class = Cache.MEMORY\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid cache_type: {cache_type}. Choose 'redis', 'memcached', or 'memory'.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Result Mock ---\n",
    "class ResultMock:\n",
    "    # Same as before (no changes needed)\n",
    "    def __init__(self, data: list) -> None:\n",
    "        self._data = data\n",
    "\n",
    "    def all(self) -> list:\n",
    "        return self._data\n",
    "\n",
    "    def first(self) -> Optional[Any]:\n",
    "        return self._data[0] if self._data else None\n",
    "\n",
    "    def scalar(self) -> Optional[Any]:\n",
    "        return self._data[0][0] if self._data else None\n",
    "\n",
    "    def scalar_one(self) -> Any:\n",
    "        if len(self._data) != 1:\n",
    "            raise (\n",
    "                MultipleResultsFound() if len(self._data) > 1 else NoResultFound()\n",
    "            )\n",
    "        return self._data[0][0]\n",
    "\n",
    "    def scalar_one_or_none(self) -> Optional[Any]:\n",
    "        if len(self._data) > 1:\n",
    "            raise MultipleResultsFound()\n",
    "        return self._data[0][0] if self._data else None\n",
    "\n",
    "    def one(self) -> Any:\n",
    "        if len(self._data) != 1:\n",
    "            raise (\n",
    "                MultipleResultsFound() if len(self._data) > 1 else NoResultFound()\n",
    "            )\n",
    "        return self._data[0]\n",
    "\n",
    "    def one_or_none(self) -> Optional[Any]:\n",
    "        if len(self._data) > 1:\n",
    "            raise MultipleResultsFound()\n",
    "        return self._data[0] if self._data else None\n",
    "\n",
    "    def __iter__(self) -> Any:\n",
    "        return iter(self._data)\n",
    "\n",
    "    def partitions(self, size: Optional[int] = None) -> list[Any]:\n",
    "        \"\"\"\n",
    "        Mock implementation of partitions method\n",
    "        \"\"\"\n",
    "        if size is None:\n",
    "            yield self._data\n",
    "        else:\n",
    "            for i in range(0, len(self._data), size):\n",
    "                yield self._data[i:i + size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cached Async Session ---\n",
    "class CachedAsyncSession(AsyncSession):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        cache_config: CacheConfig,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cache_config = cache_config\n",
    "        self.cache = Cache(\n",
    "            cache_class=cache_config.cache_class,\n",
    "            endpoint=cache_config.endpoint,\n",
    "            port=cache_config.port,\n",
    "            db=cache_config.db,\n",
    "            serializer=cache_config.serializer,\n",
    "            ttl=cache_config.ttl,\n",
    "            **cache_config.aiocache_kwargs,\n",
    "        )\n",
    "\n",
    "    async def execute(  # type: ignore[override]\n",
    "        self,\n",
    "        statement: Union[Select, TextClause, FunctionElement],\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[Result, ResultMock]:\n",
    "        if isinstance(statement, Select) or isinstance(\n",
    "            statement, FunctionElement\n",
    "        ):\n",
    "            cache_key = self._generate_cache_key(statement, **kwargs)\n",
    "\n",
    "            async def _execute_and_cache(\n",
    "                statement: Union[Select, TextClause, FunctionElement],\n",
    "                *args: Any,\n",
    "                **kwargs: Any\n",
    "            ) -> Any:\n",
    "                result = await super(CachedAsyncSession, self).execute(statement, *args, **kwargs)\n",
    "                return [list(row) for row in result.all()]\n",
    "\n",
    "            result_list = await cached(\n",
    "                ttl=self.cache_config.ttl,\n",
    "                cache=self.cache_config.cache_class,\n",
    "                key=cache_key,\n",
    "                serializer=self.cache_config.serializer,\n",
    "            )(_execute_and_cache)(statement, *args, **kwargs)\n",
    "            return ResultMock(result_list)\n",
    "        else:\n",
    "            return await super().execute(statement, *args, **kwargs)\n",
    "        \n",
    "    def _generate_cache_key(\n",
    "        self, statement: Union[Select, TextClause, FunctionElement], **kwargs: Any\n",
    "    ) -> str:\n",
    "        if isinstance(statement, Select):\n",
    "            compiled_statement = str(\n",
    "                statement.compile(compile_kwargs={\"literal_binds\": True})\n",
    "            )\n",
    "        else:\n",
    "            compiled_statement = str(statement)\n",
    "\n",
    "        params_str = json.dumps(kwargs, sort_keys=True)\n",
    "        combined_str = f\"{compiled_statement}:{params_str}\"\n",
    "        return \"db_cache:\" + hashlib.sha256(combined_str.encode()).hexdigest()\n",
    "\n",
    "    async def invalidate_cache(self, key_pattern: str) -> None:\n",
    "        \"\"\"\n",
    "        Invalidates cache keys matching a pattern.\n",
    "\n",
    "        Args:\n",
    "            key_pattern: The pattern to match (e.g., \"db_cache:*\", \"user:*:profile\").\n",
    "        \"\"\"\n",
    "        if self.cache_config.cache_type == \"redis\":\n",
    "            # Get the underlying Redis client\n",
    "            redis_client = self.cache.client  # No need to await\n",
    "\n",
    "            # Iterate over keys matching the pattern and delete them\n",
    "            async for key in redis_client.scan_iter(key_pattern):\n",
    "                await redis_client.delete(key)\n",
    "        else:\n",
    "            # For memcached or in-memory, you might need a different invalidation\n",
    "            # strategy or to clear the entire cache if key patterns aren't supported.\n",
    "            await self.cache.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from aiocache import Cache, cached\n",
    "from aiocache.serializers import JsonSerializer, PickleSerializer\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.engine import Result\n",
    "from sqlalchemy.exc import MultipleResultsFound, NoResultFound\n",
    "from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "from sqlalchemy.sql import Select\n",
    "from sqlalchemy.sql.elements import TextClause\n",
    "from sqlalchemy.sql.expression import FunctionElement\n",
    "from sqlalchemy import Column, Integer, String, select\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    email = Column(String)\n",
    "\n",
    "async def setup_db_and_cache():\n",
    "    # Database setup (using SQLite)\n",
    "    engine = create_async_engine(\"sqlite+aiosqlite:///:memory:\", echo=True)\n",
    "\n",
    "    # Cache configuration (using Redis)\n",
    "    cache_config = CacheConfig(\n",
    "        cache_type=\"redis\",\n",
    "        endpoint=\"127.0.0.1\",\n",
    "        port=6379,\n",
    "        serializer=\"pickle\",\n",
    "        ttl=60,\n",
    "    )\n",
    "\n",
    "    async_session = sessionmaker(\n",
    "        engine, expire_on_commit=False, class_=CachedAsyncSession, cache_config=cache_config\n",
    "    )\n",
    "\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "    return async_session, cache_config\n",
    "\n",
    "async def run_queries(async_session):\n",
    "    async with async_session() as session:\n",
    "        async with session.begin():\n",
    "            session.add_all([\n",
    "                User(name='user1', email='user1@example.com'),\n",
    "                User(name='user2', email='user2@example.com'),\n",
    "                User(name='user3', email='user3@example.com'),\n",
    "            ])\n",
    "        # Example with parameters\n",
    "        stmt = select(User).where(User.name == \"user1\")\n",
    "        result = await session.execute(stmt)\n",
    "        user = result.first()\n",
    "        print(\"First Query Result (might be from DB):\", user)\n",
    "\n",
    "        # Second query - should be from cache\n",
    "        stmt = select(User).where(User.name == \"user1\")\n",
    "        result = await session.execute(stmt)\n",
    "        user = result.first()\n",
    "        print(\"Second Query Result (should be from cache):\", user)\n",
    "\n",
    "        # Example to invalidation cache.\n",
    "        await session.invalidate_cache(\"db_cache:*\")\n",
    "\n",
    "        # This query is after invalidation and will hit the database\n",
    "        stmt = select(User).where(User.name == \"user1\")\n",
    "        result = await session.execute(stmt)\n",
    "        user = result.first()\n",
    "        print(\"Third Query Result (after invalidation, from DB):\", user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the greenlet library is required to use this function. No module named 'greenlet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m async_session, cache_config = \u001b[38;5;28;01mawait\u001b[39;00m setup_db_and_cache()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36msetup_db_and_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m cache_config = CacheConfig(\n\u001b[32m     33\u001b[39m     cache_type=\u001b[33m\"\u001b[39m\u001b[33mredis\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     endpoint=\u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     ttl=\u001b[32m60\u001b[39m,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     40\u001b[39m async_session = sessionmaker(\n\u001b[32m     41\u001b[39m     engine, expire_on_commit=\u001b[38;5;28;01mFalse\u001b[39;00m, class_=CachedAsyncSession, cache_config=cache_config\n\u001b[32m     42\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m engine.begin() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m conn.run_sync(Base.metadata.create_all)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m async_session, cache_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-x86_64-none/lib/python3.13/contextlib.py:214\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/fastapi-cacheops/.venv/lib/python3.13/site-packages/sqlalchemy/ext/asyncio/engine.py:1063\u001b[39m, in \u001b[36mAsyncEngine.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a context manager which when entered will deliver an\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[33;03m:class:`_asyncio.AsyncConnection` with an\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03m:class:`_asyncio.AsyncTransaction` established.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1059\u001b[39m \n\u001b[32m   1060\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1061\u001b[39m conn = \u001b[38;5;28mself\u001b[39m.connect()\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m conn:\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m conn.begin():\n\u001b[32m   1065\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m conn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/fastapi-cacheops/.venv/lib/python3.13/site-packages/sqlalchemy/ext/asyncio/base.py:121\u001b[39m, in \u001b[36mStartableContext.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _T_co:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start(is_ctxmanager=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/fastapi-cacheops/.venv/lib/python3.13/site-packages/sqlalchemy/ext/asyncio/engine.py:273\u001b[39m, in \u001b[36mAsyncConnection.start\u001b[39m\u001b[34m(self, is_ctxmanager)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sync_connection:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.InvalidRequestError(\u001b[33m\"\u001b[39m\u001b[33mconnection is already started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.sync_connection = \u001b[38;5;28mself\u001b[39m._assign_proxied(\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mgreenlet_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msync_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m )\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/fastapi-cacheops/.venv/lib/python3.13/site-packages/sqlalchemy/util/concurrency.py:99\u001b[39m, in \u001b[36mgreenlet_spawn\u001b[39m\u001b[34m(fn, *args, **kw)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgreenlet_spawn\u001b[39m(fn, *args, **kw):  \u001b[38;5;66;03m# type: ignore  # noqa: F811\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[43m_not_implemented\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/fastapi-cacheops/.venv/lib/python3.13/site-packages/sqlalchemy/util/concurrency.py:79\u001b[39m, in \u001b[36m_not_implemented\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_greenlet:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     80\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mthe greenlet library is required to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % greenlet_error\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m greenlet_error\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: the greenlet library is required to use this function. No module named 'greenlet'"
     ]
    }
   ],
   "source": [
    "async_session, cache_config = await setup_db_and_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-10 02:04:13,317 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-12-10 02:04:13,322 INFO sqlalchemy.engine.Engine INSERT INTO users (name, email) VALUES (?, ?) RETURNING id\n",
      "2024-12-10 02:04:13,325 INFO sqlalchemy.engine.Engine [generated in 0.00036s (insertmanyvalues) 1/3 (ordered; batch not supported)] ('user1', 'user1@example.com')\n",
      "2024-12-10 02:04:13,332 INFO sqlalchemy.engine.Engine INSERT INTO users (name, email) VALUES (?, ?) RETURNING id\n",
      "2024-12-10 02:04:13,333 INFO sqlalchemy.engine.Engine [insertmanyvalues 2/3 (ordered; batch not supported)] ('user2', 'user2@example.com')\n",
      "2024-12-10 02:04:13,337 INFO sqlalchemy.engine.Engine INSERT INTO users (name, email) VALUES (?, ?) RETURNING id\n",
      "2024-12-10 02:04:13,338 INFO sqlalchemy.engine.Engine [insertmanyvalues 3/3 (ordered; batch not supported)] ('user3', 'user3@example.com')\n",
      "2024-12-10 02:04:13,345 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2024-12-10 02:04:13,356 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-12-10 02:04:13,359 INFO sqlalchemy.engine.Engine SELECT users.id, users.name, users.email \n",
      "FROM users \n",
      "WHERE users.name = ?\n",
      "2024-12-10 02:04:13,361 INFO sqlalchemy.engine.Engine [generated in 0.00236s] ('user1',)\n",
      "First Query Result (might be from DB): [<__main__.User object at 0x10d896050>]\n",
      "Second Query Result (should be from cache): [<__main__.User object at 0x10d89e510>]\n",
      "2024-12-10 02:04:13,384 INFO sqlalchemy.engine.Engine SELECT users.id, users.name, users.email \n",
      "FROM users \n",
      "WHERE users.name = ?\n",
      "2024-12-10 02:04:13,386 INFO sqlalchemy.engine.Engine [cached since 0.02704s ago] ('user1',)\n",
      "Third Query Result (after invalidation, from DB): [<__main__.User object at 0x10d8a5290>]\n",
      "2024-12-10 02:04:13,393 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "await run_queries(async_session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
